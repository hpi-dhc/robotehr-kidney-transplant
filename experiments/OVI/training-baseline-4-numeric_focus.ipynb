{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../00_default_options.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-06-11 20:30:11,804] INFO in __init__: RobotEHR API\n"
     ]
    }
   ],
   "source": [
    "from robotehr.api.training import get_training_results\n",
    "from robotehr.pipelines import training\n",
    "from robotehr.models.cohort import Cohort, OnsetDataFrame\n",
    "from robotehr.models.data import FeaturePipeline\n",
    "from robotehr.pipelines.supporters.preprocessing import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort = Cohort.load(id=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "onset_dataframe = OnsetDataFrame.load(id=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_pipeline = FeaturePipeline.load(id=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import morpher.config\n",
    "from morpher.jobs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexes = [\n",
    "    'any__Diagnosis__',\n",
    "    #'any__Procedure__', \n",
    "    #'any__Drug__', \n",
    "    #'any__Material__', \n",
    "    #'any__Encounter__', \n",
    "    #'any__AlcoholUse__', \n",
    "    #'any__DrugUse__', \n",
    "    #'any__TobaccoUse__',\n",
    "    'min__height__', \n",
    "    'min__weight__', \n",
    "    'min__vitalsign__', \n",
    "    #'min__measurement__', \n",
    "    'min__labvalue__', \n",
    "    'max__height__', \n",
    "    'max__weight__', \n",
    "    'max__vitalsign__', \n",
    "    #'max__measurement__', \n",
    "    'max__labvalue__'\n",
    "]\n",
    "\n",
    "column_selector = \"\"\n",
    "for r in regexes:\n",
    "    column_selector += r + \"|\"\n",
    "column_selector = column_selector[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataLoader(DataLoader):\n",
    "    def transform(self, X, y):\n",
    "        from sklearn.preprocessing import OneHotEncoder\n",
    "        from enum import Enum\n",
    "\n",
    "        # remove unused features\n",
    "        del X['medical_record_number']\n",
    "        del X['mother_account_number']\n",
    "        del X['date_of_birth']\n",
    "        del X['month_of_birth']\n",
    "        del X['patient_ethnic_group']\n",
    "        del X['religion']\n",
    "        del X['address_zip']\n",
    "        del X['deceased_indicator']\n",
    "        del X['marital_status_code']\n",
    "\n",
    "        class RaceType(str, Enum):\n",
    "            AFRICAN = 'African'\n",
    "            AMERICAN_BLACK = 'Black or African-American'\n",
    "            AMERICAN_NATIVE = 'Native American'\n",
    "            ASIAN = 'Asian'\n",
    "            ASIAN_PACIFIC = 'Asian Pacific'\n",
    "            ASIAN_INDIAN = 'Asian Indian'\n",
    "            ASIAN_CHINESE = 'Asian Chinese'\n",
    "            HISPANIC = 'Hispanic or Latino'\n",
    "            OTHER = 'Other'\n",
    "            WHITE = 'White'\n",
    "\n",
    "        RACE_MAPPING = {\n",
    "            RaceType.AFRICAN: [\n",
    "                'Cape Verdian',\n",
    "                'Congolese',\n",
    "                'Eritrean',\n",
    "                'Ethiopian',\n",
    "                'Gabonian',\n",
    "                'Ghanaian',\n",
    "                'Guinean',\n",
    "                'Ivory Coastian',\n",
    "                'Kenyan',\n",
    "                'Liberian',\n",
    "                'Madagascar',\n",
    "                'Malian',\n",
    "                'Nigerian',\n",
    "                'Other: East African',\n",
    "                'Other: North African',\n",
    "                'Other: South African',\n",
    "                'Other: West African',\n",
    "                'Senegalese',\n",
    "                'Sierra Leonean',\n",
    "                'Somalian',\n",
    "                'Sudanese',\n",
    "                'Tanzanian',\n",
    "                'Togolese',\n",
    "                'Ugandan',\n",
    "                'Zimbabwean'\n",
    "            ],\n",
    "            RaceType.AMERICAN_BLACK: [\n",
    "                'African American (Black)',\n",
    "                'African-American',\n",
    "                'Black Or African-American',\n",
    "                'Black or African - American',\n",
    "            ],\n",
    "            RaceType.AMERICAN_NATIVE: [\n",
    "                'American (Indian/Alaskan)',\n",
    "                'Native American'\n",
    "            ],\n",
    "            RaceType.ASIAN: [\n",
    "                'Asian',\n",
    "                'Bangladeshi',\n",
    "                'Bhutanese',\n",
    "                'Burmese',\n",
    "                'Cambodian',\n",
    "                'Hmong',\n",
    "                'Indonesian',\n",
    "                'Japanese',\n",
    "                'Korean',\n",
    "                'Laotian',\n",
    "                'Malaysian',\n",
    "                'Maldivian',\n",
    "                'Nepalese',\n",
    "                'Okinawan',\n",
    "                'Pakistani',\n",
    "                'Singaporean',\n",
    "                'Taiwanese',\n",
    "                'Thai',\n",
    "                'Vietnamese',\n",
    "                'Yapese'\n",
    "            ],\n",
    "            RaceType.ASIAN_PACIFIC: [\n",
    "                'Asian (Pacific Islander)',\n",
    "                'Carolinian',\n",
    "                'Chamorro',\n",
    "                'Chuukese',\n",
    "                'Fijian',\n",
    "                'Filipino',\n",
    "                'Guamanian',\n",
    "                'Guamanian Or Chamorro',\n",
    "                'Guamanian or Chamorro',\n",
    "                'Iwo Jiman',\n",
    "                'Kiribati',\n",
    "                'Kosraean',\n",
    "                'Mariana Islander',\n",
    "                'Marshallese',\n",
    "                'Melanesian',\n",
    "                'Micronesian',\n",
    "                'Native Hawaiian',\n",
    "                'New Hebrides',\n",
    "                'Other Pacific Islander',\n",
    "                'Pacific Islander',\n",
    "                'Palauan',\n",
    "                'Pohnpeian',\n",
    "                'Polynesian',\n",
    "                'Saipanese',\n",
    "                'Samoan',\n",
    "                'Papua New Guinean',\n",
    "                'Tahitian',\n",
    "                'Tokelauan',\n",
    "                'Tongan'\n",
    "            ],\n",
    "            RaceType.ASIAN_INDIAN: [\n",
    "                'Asian Indian',\n",
    "                'Sri Lankan',\n",
    "                'Sri lankan',\n",
    "                'West Indian'\n",
    "            ],\n",
    "            RaceType.ASIAN_CHINESE: [\n",
    "                'Chinese',\n",
    "            ],\n",
    "            RaceType.HISPANIC: [\n",
    "                'Barbadian',\n",
    "                'Dominica Islander',\n",
    "                'Grenadian',\n",
    "                'Haitian',\n",
    "                'Hispanic/Latino',\n",
    "                'Jamaican',\n",
    "                'St Vincentian',\n",
    "                'Trinidadian'\n",
    "            ],\n",
    "            RaceType.OTHER: [\n",
    "                '',\n",
    "                'Aa',\n",
    "                'Ab',\n",
    "                'Af',\n",
    "                'Ag',\n",
    "                'Ak',\n",
    "                'Al',\n",
    "                'Ap',\n",
    "                'Ar',\n",
    "                'Av',\n",
    "                'Ay',\n",
    "                'B',\n",
    "                'B1',\n",
    "                'B2',\n",
    "                'B3',\n",
    "                'B4',\n",
    "                'B5',\n",
    "                'B6',\n",
    "                'B7',\n",
    "                'B8',\n",
    "                'B9',\n",
    "                'Ba',\n",
    "                'Bb',\n",
    "                'Bc',\n",
    "                'Bd',\n",
    "                'Be',\n",
    "                'Bf',\n",
    "                'Bg',\n",
    "                'Bh',\n",
    "                'Bj',\n",
    "                'Bk',\n",
    "                'Bm',\n",
    "                'Bn',\n",
    "                'Bo',\n",
    "                'Bp',\n",
    "                'Bq',\n",
    "                'Br',\n",
    "                'Bs',\n",
    "                'Bt',\n",
    "                'Bu',\n",
    "                'Bv',\n",
    "                'Bw',\n",
    "                'Bx',\n",
    "                'By',\n",
    "                'Bz',\n",
    "                'I',\n",
    "                'MSDW_NOT APPLICABLE',\n",
    "                'MSDW_OTHER',\n",
    "                'MSDW_UNKNOWN',\n",
    "                'NOT AVAILABLE',\n",
    "                'Non Hispanic',\n",
    "                'O',\n",
    "                'Other',\n",
    "                'Pk',\n",
    "                'Pl',\n",
    "                'Pm',\n",
    "                'Po',\n",
    "                'Ps',\n",
    "                'Pv',\n",
    "                'U',\n",
    "                'Unk',\n",
    "                'Unknown',\n",
    "                'W'\n",
    "            ],\n",
    "            RaceType.WHITE: [\n",
    "                'Caucasian (White)',\n",
    "                'White'\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        # value mapping\n",
    "        X['race'] = (\n",
    "            X.race.map({\n",
    "                label: cat for cat, labels in RACE_MAPPING.items()\n",
    "                for label in labels\n",
    "            }).astype(pd.api.types.CategoricalDtype(RaceType))\n",
    "        )\n",
    "\n",
    "        # pre-encoding nan handling\n",
    "        for column in ['gender', 'race']:\n",
    "            enc = OneHotEncoder(sparse=False)\n",
    "            transformed_data = enc.fit_transform(X[[column]])\n",
    "            transformed_columns = pd.DataFrame(\n",
    "                data=transformed_data, \n",
    "                columns=[f'{column}_{c}' for c in enc.categories_[0]]\n",
    "            )\n",
    "            X = X.join(transformed_columns)\n",
    "            del X[column]\n",
    "        X[X.columns[X.columns.str.contains('any')]] = X[X.columns[X.columns.str.contains('any')]].fillna(False)\n",
    "        return X, y\n",
    "        \n",
    "    def transform_training_data(self, X_train, y_train):\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        scaler = StandardScaler()\n",
    "        self.objects['scaler'] = scaler\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        \n",
    "        from sklearn.impute import SimpleImputer\n",
    "        imputer = SimpleImputer()\n",
    "        self.objects['imputer'] = imputer\n",
    "        X_train_imputed = imputer.fit_transform(X_train_scaled)\n",
    "        \n",
    "        X_train = pd.DataFrame(\n",
    "            data=X_train_imputed, \n",
    "            columns=X_train.columns,\n",
    "            index=X_train.index\n",
    "        )\n",
    "        return X_train, y_train\n",
    "    \n",
    "    def transform_test_data(self, X_test, y_test):\n",
    "        scaler = self.objects['scaler']\n",
    "        imputer = self.objects['imputer']\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        X_test_imputed = imputer.fit_transform(X_test_scaled)\n",
    "        \n",
    "        X_test = pd.DataFrame(\n",
    "            data=X_test_imputed, \n",
    "            columns=X_test.columns,\n",
    "            index=X_test.index\n",
    "        )\n",
    "        return X_test, y_test\n",
    "        \n",
    "\n",
    "data_loader = CustomDataLoader(column_selector=column_selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds_numeric = [(x / 100) for x in range(5, 10, 5)]\n",
    "observation_windows_numeric = [[x, -1] for x in range(-361, -331, 30)]\n",
    "thresholds_occurring = [0.05]\n",
    "observation_windows_occurring = [[-361, -1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = product(\n",
    "    thresholds_numeric,\n",
    "    observation_windows_numeric,\n",
    "    thresholds_occurring,\n",
    "    observation_windows_occurring\n",
    ")\n",
    "\n",
    "configs = [x for x in iterator]\n",
    "targets = ['other_viral_infection_onset_from_0_days_after_to_365_days_after']\n",
    "algorithms = [morpher.config.algorithms.GBDT, morpher.config.algorithms.RF, morpher.config.algorithms.DT, morpher.config.algorithms.LR]\n",
    "samplers = [morpher.config.samplers.RANDOM, morpher.config.samplers.URANDOM, morpher.config.samplers.BORDERLINE, morpher.config.samplers.SMOTE, morpher.config.samplers.NOSAMPLER]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Columns (11,18,19,20,24,35,64,83,90,92,94,96,97,106,114,117,120,125,126,133,136,140,141,142,143,144,150,178,235,244,248,288,311,317,320,325,326,330,334,341,353,359,360,361,365,402,421,425,430,434,436,439,446) have mixed types. Specify dtype option on import or set low_memory=False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data for <fiber.condition.mrns.MRNs object at 0x7fdd0e5279e8>\n",
      "Fetching data for Patient (...)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d0f7f17cfae4e798261c1599a8a58ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Data with input dtype bool, float64 were all converted to float64 by StandardScaler.\n",
      "Data with input dtype bool, float64 were all converted to float64 by StandardScaler.\n",
      "Data with input dtype bool, float64 were all converted to float64 by StandardScaler.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Training of model 'GradientBoostingClassifier' started.\n",
      "*** Training of classifier ready. Time elapsed: 635.497ms\n",
      "\n",
      "*** Training of model 'GradientBoostingClassifier' started.\n"
     ]
    }
   ],
   "source": [
    "pipeline = training.execute(\n",
    "    comment='baseline-4 ovi tw binning NEW',\n",
    "    version='234.0.0',\n",
    "    cohort=cohort,\n",
    "    onset_dataframe=onset_dataframe,\n",
    "    feature_pipeline=feature_pipeline,\n",
    "    data_loader=data_loader,\n",
    "    observation_iterator=configs,\n",
    "    targets=targets,\n",
    "    algorithms=algorithms,\n",
    "    samplers=samplers,\n",
    "    feature_type_occurring=\"occurring\",\n",
    "    feature_type_numeric=\"numeric_binned\",\n",
    "    bin_size=30,\n",
    "    rfe__run=True,\n",
    "    rfe__step_size=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipeline.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_training_results(pipeline.id, sort_by='auc_roc_mean', metrics=['auc_roc_mean', 'precision_mean', 'auc_prc_mean', 'recall_mean'], n_rows=10000, response_type=\"pandas\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
